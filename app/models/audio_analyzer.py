import librosa
import numpy as np
import soundfile as sf
from transformers import pipeline
import torch
import tempfile
import os
import time  # ADDED - was missing
from datetime import datetime  # ADDED - was missing
from typing import Dict, Optional, Tuple, List
import logging
import re
import warnings

# Suppress librosa warnings for cleaner output
warnings.filterwarnings("ignore", category=UserWarning, module="librosa")

logger = logging.getLogger(__name__)

class AudioAnalyzer:
    def __init__(self):
        """Initialize audio analysis models - English only, optimized for mental health"""
        try:
            # Enhanced speech-to-text model
            logger.info("Loading Whisper speech-to-text model...")
            self.speech_to_text = pipeline(
                "automatic-speech-recognition",
                model="openai/whisper-base",
                return_timestamps=True,
                device=0 if torch.cuda.is_available() else -1
            )
            
            # Emotion recognition model with fallback handling
            logger.info("Loading emotion recognition model...")
            try:
                self.emotion_classifier = pipeline(
                    "audio-classification",
                    model="ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition",
                    device=0 if torch.cuda.is_available() else -1
                )
                logger.info("Primary emotion model loaded successfully")
            except Exception as e:
                logger.warning(f"Primary emotion model failed: {e}, using fallback")
                self.emotion_classifier = None
            
            # Audio processing parameters
            self.sample_rate = 16000
            self.frame_length = int(0.025 * self.sample_rate)  # 25ms frames
            self.hop_length = int(0.01 * self.sample_rate)     # 10ms hop
            self.min_audio_duration = 1.0  # Minimum 1 second
            self.max_audio_duration = 300.0  # Maximum 5 minutes
            
            # ENHANCED mental health keywords - English only, more comprehensive
            self.mental_health_keywords = {
                'depression': [
                    'sad', 'hopeless', 'empty', 'worthless', 'tired', 'fatigue', 
                    'sleep problems', 'insomnia', 'death', 'suicide', 'end it all', 
                    'can\'t go on', 'no energy', 'exhausted', 'meaningless', 'pointless', 
                    'give up', 'hate myself', 'burden', 'better off dead', 'no hope',
                    'nothing matters', 'feel like dying', 'want to disappear', 
                    'life is not worth living', 'depressed', 'down', 'low mood'
                ],
                'anxiety': [
                    'anxious', 'worried', 'nervous', 'panic', 'fear', 'scared',
                    'restless', 'tension', 'heart racing', 'overthinking', 'catastrophic',
                    'worst case', 'can\'t relax', 'on edge', 'jittery', 'panic attack',
                    'heart pounding', 'shortness of breath', 'dizzy', 'sweating',
                    'trembling', 'shaking', 'paranoid', 'hypervigilant', 'constant worry'
                ],
                'stress': [
                    'stressed', 'overwhelmed', 'pressure', 'exhausted', 
                    'burned out', 'can\'t cope', 'too much', 'breaking point',
                    'overloaded', 'swamped', 'drowning', 'suffocating', 
                    'cracking under pressure', 'at my limit', 'falling apart',
                    'can\'t handle', 'under pressure', 'breaking down'
                ],
                'ptsd': [
                    'flashback', 'nightmare', 'trauma', 'triggered', 'avoidance',
                    'hypervigilant', 'memories', 'intrusive', 'haunted', 'reliving',
                    'vivid memories', 'can\'t forget', 'happened again', 'jumpy',
                    'startled', 'on guard', 'traumatic', 'disturbing memories'
                ],
                'self_harm': [
                    'cut myself', 'hurt myself', 'self harm', 'cutting', 'razor',
                    'burn myself', 'punish myself', 'deserve pain', 'self injury',
                    'harm myself', 'cut my arms', 'cut my wrists', 'self mutilation'
                ],
                'bipolar': [
                    'manic', 'mania', 'mood swing', 'high energy', 'euphoric',
                    'racing thoughts', 'up and down', 'extreme', 'rollercoaster',
                    'mood changes', 'emotional swings', 'highs and lows'
                ]
            }
            
            logger.info("Audio analyzer initialized successfully")
            
        except Exception as e:
            logger.error(f"Critical error initializing audio analyzer: {str(e)}")
            raise RuntimeError(f"Failed to initialize AudioAnalyzer: {str(e)}")
    
    def validate_audio_file(self, audio_path: str) -> bool:
        """Validate audio file before processing"""
        try:
            if not os.path.exists(audio_path):
                logger.error(f"Audio file does not exist: {audio_path}")
                return False
            
            if os.path.getsize(audio_path) == 0:
                logger.error("Audio file is empty")
                return False
            
            # Try to load a small portion to validate format
            try:
                test_audio, _ = librosa.load(audio_path, duration=1.0, sr=self.sample_rate)
                if len(test_audio) == 0:
                    logger.error("Audio file contains no audio data")
                    return False
            except Exception as e:
                logger.error(f"Invalid audio format: {str(e)}")
                return False
            
            return True
            
        except Exception as e:
            logger.error(f"Audio validation failed: {str(e)}")
            return False
    
    def load_audio(self, audio_path: str, sr: int = None) -> Tuple[np.ndarray, int]:
        """Load and validate audio file with enhanced error handling"""
        try:
            if sr is None:
                sr = self.sample_rate
            
            # Validate file first
            if not self.validate_audio_file(audio_path):
                raise ValueError("Invalid audio file")
            
            # Load audio with librosa
            audio, original_sr = librosa.load(audio_path, sr=sr)
            
            # Validate audio duration
            duration = len(audio) / sr
            if duration < self.min_audio_duration:
                logger.warning(f"Audio too short: {duration:.2f}s (minimum: {self.min_audio_duration}s)")
            elif duration > self.max_audio_duration:
                logger.warning(f"Audio too long: {duration:.2f}s (maximum: {self.max_audio_duration}s), truncating")
                max_samples = int(self.max_audio_duration * sr)
                audio = audio[:max_samples]
            
            # Normalize audio to prevent clipping
            if np.max(np.abs(audio)) > 0:
                audio = audio / np.max(np.abs(audio)) * 0.95
            
            logger.info(f"Audio loaded successfully: {duration:.2f}s, {sr}Hz")
            return audio, sr
            
        except Exception as e:
            logger.error(f"Failed to load audio from {audio_path}: {str(e)}")
            raise
    
    def extract_audio_features(self, audio: np.ndarray, sr: int) -> Dict:
        """Extract comprehensive audio features optimized for mental health analysis"""
        try:
            features = {}
            
            # Basic temporal features
            features['duration'] = float(len(audio) / sr)
            features['sample_rate'] = sr
            features['rms_energy'] = float(np.sqrt(np.mean(audio**2)))
            features['zero_crossing_rate'] = float(np.mean(librosa.feature.zero_crossing_rate(audio)))
            
            # Enhanced spectral features
            spectral_centroids = librosa.feature.spectral_centroid(y=audio, sr=sr)[0]
            features['spectral_centroid_mean'] = float(np.mean(spectral_centroids))
            features['spectral_centroid_std'] = float(np.std(spectral_centroids))
            features['spectral_centroid_range'] = float(np.max(spectral_centroids) - np.min(spectral_centroids))
            
            spectral_rolloff = librosa.feature.spectral_rolloff(y=audio, sr=sr)[0]
            features['spectral_rolloff_mean'] = float(np.mean(spectral_rolloff))
            
            spectral_bandwidth = librosa.feature.spectral_bandwidth(y=audio, sr=sr)[0]
            features['spectral_bandwidth_mean'] = float(np.mean(spectral_bandwidth))
            
            # MFCC features (critical for speech emotion recognition)
            mfccs = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=13)
            for i in range(13):
                features[f'mfcc_{i+1}_mean'] = float(np.mean(mfccs[i]))
                features[f'mfcc_{i+1}_std'] = float(np.std(mfccs[i]))
            
            # Enhanced pitch analysis (crucial for mental health assessment)
            try:
                pitches, magnitudes = librosa.piptrack(y=audio, sr=sr, threshold=0.1)
                pitch_values = pitches[pitches > 0]
                
                if len(pitch_values) > 0:
                    features['pitch_mean'] = float(np.mean(pitch_values))
                    features['pitch_std'] = float(np.std(pitch_values))
                    features['pitch_min'] = float(np.min(pitch_values))
                    features['pitch_max'] = float(np.max(pitch_values))
                    features['pitch_range'] = features['pitch_max'] - features['pitch_min']
                    features['pitch_variation_coefficient'] = features['pitch_std'] / features['pitch_mean'] if features['pitch_mean'] > 0 else 0.0
                else:
                    features.update({
                        'pitch_mean': 0.0, 'pitch_std': 0.0, 'pitch_min': 0.0, 
                        'pitch_max': 0.0, 'pitch_range': 0.0, 'pitch_variation_coefficient': 0.0
                    })
            except Exception as e:
                logger.warning(f"Pitch analysis failed: {e}")
                features.update({
                    'pitch_mean': 0.0, 'pitch_std': 0.0, 'pitch_min': 0.0, 
                    'pitch_max': 0.0, 'pitch_range': 0.0, 'pitch_variation_coefficient': 0.0
                })
            
            # Tempo and rhythm analysis
            try:
                tempo, beats = librosa.beat.beat_track(y=audio, sr=sr)
                features['tempo'] = float(tempo)
                features['beat_count'] = len(beats)
                features['rhythm_regularity'] = float(np.std(np.diff(beats))) if len(beats) > 1 else 0.0
            except Exception as e:
                logger.warning(f"Tempo analysis failed: {e}")
                features.update({'tempo': 0.0, 'beat_count': 0, 'rhythm_regularity': 0.0})
            
            # Chroma and tonal features
            try:
                chroma = librosa.feature.chroma_stft(y=audio, sr=sr)
                features['chroma_mean'] = float(np.mean(chroma))
                features['chroma_std'] = float(np.std(chroma))
                features['chroma_energy'] = float(np.sum(chroma))
            except Exception as e:
                logger.warning(f"Chroma analysis failed: {e}")
                features.update({'chroma_mean': 0.0, 'chroma_std': 0.0, 'chroma_energy': 0.0})
            
            # Spectral contrast (voice quality indicator)
            try:
                spectral_contrast = librosa.feature.spectral_contrast(y=audio, sr=sr)
                features['spectral_contrast_mean'] = float(np.mean(spectral_contrast))
                features['spectral_contrast_std'] = float(np.std(spectral_contrast))
            except Exception as e:
                logger.warning(f"Spectral contrast analysis failed: {e}")
                features.update({'spectral_contrast_mean': 0.0, 'spectral_contrast_std': 0.0})
            
            # Advanced voice quality indicators
            features['voice_stability'] = self._calculate_voice_stability(audio, sr)
            features['speech_rate'] = self._estimate_speech_rate(audio, sr)
            features['voice_intensity_variation'] = self._calculate_intensity_variation(audio, sr)
            features['silence_ratio'] = self._calculate_silence_ratio(audio, sr)
            
            logger.debug(f"Extracted {len(features)} audio features successfully")
            return features
            
        except Exception as e:
            logger.error(f"Feature extraction failed: {str(e)}")
            return {
                'error': str(e), 'duration': 0.0, 'rms_energy': 0.0,
                'pitch_mean': 0.0, 'voice_stability': 0.5
            }
    
    def _calculate_voice_stability(self, audio: np.ndarray, sr: int) -> float:
        """Calculate voice stability - indicator of emotional state"""
        try:
            frames = librosa.util.frame(audio, frame_length=self.frame_length, 
                                      hop_length=self.hop_length, axis=0)
            energy = np.sum(frames**2, axis=0)
            
            if len(energy) <= 1:
                return 0.5
            
            energy_mean = np.mean(energy)
            energy_std = np.std(energy)
            
            if energy_mean > 0:
                cv = energy_std / energy_mean
                stability = 1.0 / (1.0 + cv)
            else:
                stability = 0.5
            
            return float(np.clip(stability, 0.0, 1.0))
            
        except Exception as e:
            logger.warning(f"Voice stability calculation failed: {e}")
            return 0.5
    
    def _estimate_speech_rate(self, audio: np.ndarray, sr: int) -> float:
        """Estimate speech rate in words per minute"""
        try:
            zcr = librosa.feature.zero_crossing_rate(audio, frame_length=self.frame_length, 
                                                   hop_length=self.hop_length)[0]
            
            frames = librosa.util.frame(audio, frame_length=self.frame_length, 
                                      hop_length=self.hop_length, axis=0)
            energy = np.sum(frames**2, axis=0)
            
            speech_activity = np.mean(zcr) * np.mean(energy)
            
            duration_minutes = len(audio) / (sr * 60)
            if duration_minutes > 0:
                estimated_wpm = min((speech_activity * 150) / duration_minutes, 300.0)
            else:
                estimated_wpm = 0.0
            
            return float(estimated_wpm)
            
        except Exception as e:
            logger.warning(f"Speech rate estimation failed: {e}")
            return 0.0
    
    def _calculate_intensity_variation(self, audio: np.ndarray, sr: int) -> float:
        """Calculate intensity variation - indicator of emotional expression"""
        try:
            window_size = int(0.1 * sr)  # 100ms windows
            hop_size = int(0.05 * sr)    # 50ms hop
            
            intensities = []
            for i in range(0, len(audio) - window_size, hop_size):
                window = audio[i:i + window_size]
                intensity = np.sqrt(np.mean(window**2))
                intensities.append(intensity)
            
            if len(intensities) <= 1:
                return 0.0
            
            intensities = np.array(intensities)
            mean_intensity = np.mean(intensities)
            std_intensity = np.std(intensities)
            
            if mean_intensity > 0:
                variation = std_intensity / mean_intensity
            else:
                variation = 0.0
            
            return float(np.clip(variation, 0.0, 2.0))
            
        except Exception as e:
            logger.warning(f"Intensity variation calculation failed: {e}")
            return 0.0
    
    def _calculate_silence_ratio(self, audio: np.ndarray, sr: int) -> float:
        """Calculate ratio of silence to total audio"""
        try:
            energy = audio**2
            energy_threshold = np.mean(energy) * 0.01  # 1% of mean energy
            
            silent_samples = np.sum(energy < energy_threshold)
            total_samples = len(audio)
            
            silence_ratio = silent_samples / total_samples if total_samples > 0 else 0.0
            
            return float(np.clip(silence_ratio, 0.0, 1.0))
            
        except Exception as e:
            logger.warning(f"Silence ratio calculation failed: {e}")
            return 0.0
    
    def detect_voice_activity(self, audio: np.ndarray, sr: int) -> Dict:
        """Enhanced voice activity detection with detailed metrics"""
        try:
            frames = librosa.util.frame(audio, frame_length=self.frame_length, 
                                      hop_length=self.hop_length, axis=0)
            energy = np.sum(frames**2, axis=0)
            
            energy_mean = np.mean(energy)
            energy_std = np.std(energy)
            energy_threshold = energy_mean + (0.2 * energy_std)
            
            voice_frames = energy > energy_threshold
            
            total_frames = len(voice_frames)
            speech_frames = np.sum(voice_frames)
            speech_ratio = speech_frames / total_frames if total_frames > 0 else 0
            
            # Find speech segments
            segments = []
            in_speech = False
            start_frame = 0
            min_segment_duration = 0.1  # Minimum 100ms segments
            
            for i, is_speech in enumerate(voice_frames):
                if is_speech and not in_speech:
                    start_frame = i
                    in_speech = True
                elif not is_speech and in_speech:
                    segment_duration = (i - start_frame) * self.hop_length / sr
                    if segment_duration >= min_segment_duration:
                        segments.append((start_frame * self.hop_length / sr, i * self.hop_length / sr))
                    in_speech = False
            
            if in_speech:
                segment_duration = (len(voice_frames) - start_frame) * self.hop_length / sr
                if segment_duration >= min_segment_duration:
                    segments.append((start_frame * self.hop_length / sr, len(voice_frames) * self.hop_length / sr))
            
            # Calculate metrics
            if segments:
                segment_durations = [seg[1] - seg[0] for seg in segments]
                avg_segment_duration = np.mean(segment_durations)
                
                pause_durations = []
                for i in range(1, len(segments)):
                    pause_duration = segments[i][0] - segments[i-1][1]
                    pause_durations.append(pause_duration)
                
                avg_pause_duration = np.mean(pause_durations) if pause_durations else 0.0
                speech_continuity = np.sum(segment_durations) / (len(audio) / sr)
            else:
                avg_segment_duration = 0.0
                avg_pause_duration = 0.0
                speech_continuity = 0.0
            
            return {
                'speech_ratio': float(speech_ratio),
                'total_duration': float(len(audio) / sr),
                'speech_duration': float(speech_ratio * len(audio) / sr),
                'speech_segments': segments,
                'num_segments': len(segments),
                'avg_segment_duration': float(avg_segment_duration),
                'avg_pause_duration': float(avg_pause_duration),
                'speech_continuity': float(speech_continuity),
                'voice_activity_score': float(speech_ratio * len(segments)) if len(segments) > 0 else 0.0
            }
            
        except Exception as e:
            logger.error(f"Voice activity detection failed: {str(e)}")
            return {
                'speech_ratio': 0.5, 'total_duration': len(audio) / sr, 
                'speech_duration': len(audio) / sr * 0.5, 'speech_segments': [], 
                'num_segments': 0, 'avg_segment_duration': 0.0, 'avg_pause_duration': 0.0,
                'speech_continuity': 0.5, 'voice_activity_score': 0.0
            }
    
    def transcribe_audio(self, audio_path: str) -> Dict:
        """Enhanced transcription with robust error handling"""
        try:
            logger.info(f"Starting transcription for: {audio_path}")
            
            result = self.speech_to_text(audio_path, task="transcribe")
            
            if isinstance(result, dict):
                transcription = result.get('text', '').strip()
                timestamps = result.get('chunks', [])
            elif isinstance(result, str):
                transcription = result.strip()
                timestamps = []
            else:
                transcription = str(result).strip()
                timestamps = []
            
            # Calculate quality metrics
            confidence = self._calculate_transcription_confidence(transcription)
            quality = self._assess_transcription_quality(transcription)
            
            word_count = len(transcription.split()) if transcription else 0
            char_count = len(transcription) if transcription else 0
            
            logger.info(f"Transcription completed: {word_count} words")
            
            return {
                'transcription': transcription,
                'timestamps': timestamps,
                'confidence': confidence,
                'language': 'en',  # English only now
                'word_count': word_count,
                'character_count': char_count,
                'transcription_quality': quality,
                'is_empty': len(transcription) == 0,
                'processing_successful': True
            }
            
        except Exception as e:
            logger.error(f"Transcription failed for {audio_path}: {str(e)}")
            return {
                'transcription': '',
                'timestamps': [],
                'confidence': 0.0,
                'language': 'en',
                'word_count': 0,
                'character_count': 0,
                'transcription_quality': 'failed',
                'is_empty': True,
                'processing_successful': False,
                'error': str(e)
            }
    
    def _calculate_transcription_confidence(self, transcription: str) -> float:
        """Calculate confidence score for transcription quality"""
        if not transcription:
            return 0.0
        
        try:
            confidence = 0.3  # Base confidence
            
            word_count = len(transcription.split())
            char_count = len(transcription)
            
            if word_count >= 10:
                confidence += 0.3
            elif word_count >= 5:
                confidence += 0.2
            elif word_count >= 2:
                confidence += 0.1
            
            meaningful_chars = re.findall(r'[a-zA-Z]', transcription)
            if char_count > 0:
                meaningful_ratio = len(meaningful_chars) / char_count
                if meaningful_ratio > 0.8:
                    confidence += 0.2
                elif meaningful_ratio > 0.6:
                    confidence += 0.1
            
            words = transcription.split()
            if len(words) > 0:
                unique_words = set(words)
                uniqueness_ratio = len(unique_words) / len(words)
                
                if uniqueness_ratio > 0.8:
                    confidence += 0.1
                elif uniqueness_ratio < 0.4:
                    confidence -= 0.2
            
            if re.search(r'[.!?]', transcription):
                confidence += 0.05
            
            special_chars = re.findall(r'[^\w\s.!?,-]', transcription)
            if len(special_chars) > char_count * 0.1:
                confidence -= 0.1
            
            return float(np.clip(confidence, 0.0, 1.0))
            
        except Exception as e:
            logger.warning(f"Confidence calculation failed: {e}")
            return 0.5
    
    def _assess_transcription_quality(self, transcription: str) -> str:
        """Assess overall transcription quality"""
        if not transcription:
            return 'failed'
        
        confidence = self._calculate_transcription_confidence(transcription)
        
        if confidence > 0.85:
            return 'excellent'
        elif confidence > 0.7:
            return 'good'
        elif confidence > 0.5:
            return 'fair'
        elif confidence > 0.3:
            return 'poor'
        else:
            return 'very_poor'
    
    def analyze_emotion_from_audio(self, audio_path: str) -> Dict:
        """Enhanced emotion analysis with robust fallback mechanisms"""
        try:
            if self.emotion_classifier is None:
                logger.info("Using fallback emotion analysis")
                return self._fallback_emotion_analysis(audio_path)
            
            logger.info("Analyzing emotions using primary model")
            
            result = self.emotion_classifier(audio_path)
            
            emotions = {}
            if isinstance(result, list) and len(result) > 0:
                for item in result:
                    if isinstance(item, dict) and 'label' in item and 'score' in item:
                        emotions[item['label']] = float(item['score'])
            
            if emotions:
                top_emotion = max(emotions, key=emotions.get)
                confidence = emotions[top_emotion]
            else:
                logger.warning("No emotions detected, using neutral")
                top_emotion = 'neutral'
                confidence = 1.0
                emotions = {'neutral': 1.0}
            
            # Enhanced emotion mapping for mental health context
            mental_health_emotion_mapping = {
                'sad': 'sadness', 'angry': 'anger', 'fear': 'anxiety',
                'happy': 'joy', 'surprised': 'surprise', 'disgusted': 'disgust',
                'calm': 'calm', 'neutral': 'neutral', 'excited': 'excitement'
            }
            
            mapped_emotion = mental_health_emotion_mapping.get(top_emotion.lower(), top_emotion)
            emotional_intensity = self._calculate_emotional_intensity(emotions)
            
            return {
                'emotions': emotions,
                'dominant_emotion': mapped_emotion,
                'confidence': float(confidence),
                'model_used': 'wav2vec2-emotion',
                'emotional_intensity': emotional_intensity,
                'analysis_successful': True
            }
            
        except Exception as e:
            logger.error(f"Primary emotion analysis failed: {str(e)}")
            return self._fallback_emotion_analysis(audio_path)
    
    def _calculate_emotional_intensity(self, emotions: Dict) -> float:
        """Calculate overall emotional intensity with enhanced weighting"""
        if not emotions:
            return 0.0
        
        try:
            intensity_weights = {
                'angry': 1.8, 'fear': 1.7, 'sad': 1.6, 'disgusted': 1.4,
                'surprised': 1.2, 'happy': 1.0, 'excited': 1.3,
                'calm': 0.4, 'neutral': 0.2
            }
            
            total_intensity = 0.0
            total_weight = 0.0
            
            for emotion, score in emotions.items():
                weight = intensity_weights.get(emotion.lower(), 1.0)
                total_intensity += score * weight
                total_weight += score
            
            if total_weight > 0:
                intensity = total_intensity / total_weight
            else:
                intensity = 0.0
            
            return float(np.clip(intensity, 0.0, 2.0))
            
        except Exception as e:
            logger.warning(f"Emotional intensity calculation failed: {e}")
            return 0.5
    
    def analyze_mental_health_indicators(self, transcription: Dict, audio_features: Dict) -> Dict:
        """ENHANCED mental health analysis - FIXED for better detection"""
        try:
            mental_health_score = 0.0
            indicators = []
            detected_conditions = {}
            
            transcription_text = transcription.get('transcription', '').lower()
            transcription_quality = transcription.get('transcription_quality', 'poor')
            
            # Skip if transcription is too poor
            if transcription_quality in ['failed', 'very_poor'] or len(transcription_text.strip()) == 0:
                logger.warning("Transcription quality too poor for reliable keyword analysis")
                return self._analyze_audio_only_indicators(audio_features)
            
            # ENHANCED keyword analysis - more aggressive matching
            for condition, keywords in self.mental_health_keywords.items():
                found_keywords = []
                keyword_contexts = []
                
                # Direct keyword matching
                for keyword in keywords:
                    if keyword.lower() in transcription_text:
                        found_keywords.append(keyword)
                        context = self._extract_keyword_context(transcription_text, keyword)
                        keyword_contexts.append(context)
                        
                        # Enhanced severity weighting
                        severity_weight = self._get_keyword_severity_weight(keyword, condition)
                        mental_health_score += 0.1 * severity_weight  # Increased base score
                
                # Additional phrase matching for better detection
                if condition == 'depression':
                    depression_phrases = [
                        'feel sad', 'very sad', 'extremely sad', 'so sad', 'really sad',
                        'feel empty', 'feel hopeless', 'feel worthless', 'want to die',
                        'life is meaningless', 'no point in living', 'feel depressed',
                        'hate my life', 'nothing matters', 'feel down', 'low mood'
                    ]
                    for phrase in depression_phrases:
                        if phrase in transcription_text:
                            found_keywords.append(phrase)
                            mental_health_score += 0.15
                
                elif condition == 'anxiety':
                    anxiety_phrases = [
                        'feel anxious', 'very anxious', 'so worried', 'panic attacks',
                        'anxiety attacks', 'feel nervous', 'heart racing', 'can\'t breathe',
                        'feel scared', 'constant worry', 'overthinking', 'panic disorder'
                    ]
                    for phrase in anxiety_phrases:
                        if phrase in transcription_text:
                            found_keywords.append(phrase)
                            mental_health_score += 0.12
                
                elif condition == 'stress':
                    stress_phrases = [
                        'feel stressed', 'under pressure', 'too much stress',
                        'can\'t cope', 'overwhelmed with', 'breaking down',
                        'burned out', 'at my limit', 'falling apart'
                    ]
                    for phrase in stress_phrases:
                        if phrase in transcription_text:
                            found_keywords.append(phrase)
                            mental_health_score += 0.1
                
                elif condition == 'self_harm':
                    self_harm_phrases = [
                        'hurt myself', 'cut myself', 'harm myself', 'self injury',
                        'want to cut', 'cutting myself', 'self harm'
                    ]
                    for phrase in self_harm_phrases:
                        if phrase in transcription_text:
                            found_keywords.append(phrase)
                            mental_health_score += 0.3  # High weight for self-harm
                
                # If matches found, calculate severity
                if found_keywords:
                    # More sensitive scoring - lower threshold
                    base_severity = min(len(found_keywords) / max(len(keywords), 3), 1.0)
                    
                    # Critical keywords boost
                    critical_keywords = {
                        'depression': ['suicide', 'kill myself', 'want to die', 'end it all', 'better off dead'],
                        'anxiety': ['panic attack', 'can\'t breathe', 'heart racing'],
                        'stress': ['breaking point', 'can\'t cope', 'falling apart'],
                        'self_harm': ['cut myself', 'hurt myself', 'self harm']
                    }
                    
                    critical_matches = sum(1 for kw in found_keywords 
                                         if kw in critical_keywords.get(condition, []))
                    
                    # Enhanced severity calculation
                    context_weight = self._analyze_keyword_contexts(keyword_contexts, condition)
                    final_severity = min(base_severity + (critical_matches * 0.4) + context_weight, 1.0)
                    
                    # Ensure minimum severity for any detection
                    final_severity = max(final_severity, 0.4)  # Increased minimum
                    confidence = min(final_severity * 120, 100)  # Boosted confidence
                    
                    detected_conditions[condition] = {
                        'keywords': found_keywords[:5],
                        'severity': float(final_severity),
                        'confidence': float(confidence),
                        'critical_indicators': critical_matches > 0,
                        'context_indicators': keyword_contexts[:3]
                    }
                    
                    indicators.append(f'{condition.title()} indicators: {", ".join(found_keywords[:3])}')
            
            # Enhanced voice characteristics analysis
            voice_indicators = self._analyze_voice_characteristics(audio_features)
            mental_health_score += voice_indicators['score']
            indicators.extend(voice_indicators['indicators'])
            
            # Merge voice-based conditions
            for condition, data in voice_indicators['conditions'].items():
                if condition in detected_conditions:
                    existing = detected_conditions[condition]
                    combined_severity = (existing['severity'] + data['severity']) / 2
                    combined_confidence = max(existing['confidence'], data['confidence'])
                    
                    detected_conditions[condition].update({
                        'severity': combined_severity,
                        'confidence': combined_confidence,
                        'voice_indicators': data.get('voice_features', [])
                    })
                else:
                    detected_conditions[condition] = data
            
            # Determine primary concern
            primary_concern = self._determine_primary_concern(detected_conditions)
            
            # Calculate analysis confidence
            analysis_confidence = self._calculate_enhanced_analysis_confidence(
                transcription, audio_features, detected_conditions
            )
            
            return {
                'mental_health_score': float(min(mental_health_score, 1.0)),
                'indicators': indicators,
                'detected_conditions': detected_conditions,
                'primary_concern': primary_concern,
                'language_analyzed': 'en',
                'analysis_confidence': analysis_confidence,
                'transcription_quality_used': transcription_quality,
                'analysis_method': 'combined_text_audio'
            }
            
        except Exception as e:
            logger.error(f"Mental health analysis failed: {str(e)}")
            return self._analyze_audio_only_indicators(audio_features)
    
    def _extract_keyword_context(self, text: str, keyword: str, context_length: int = 20) -> str:
        """Extract context around a keyword for better analysis"""
        try:
            words = text.split()
            keyword_positions = [i for i, word in enumerate(words) if keyword.lower() in word.lower()]
            
            contexts = []
            for pos in keyword_positions:
                start = max(0, pos - context_length // 2)
                end = min(len(words), pos + context_length // 2)
                context = ' '.join(words[start:end])
                contexts.append(context)
            
            return ' | '.join(contexts) if contexts else keyword
            
        except Exception:
            return keyword
    
    def _get_keyword_severity_weight(self, keyword: str, condition: str) -> float:
        """Get severity weight for specific keywords"""
        high_severity_keywords = {
            'depression': ['suicide', 'kill myself', 'want to die', 'end it all', 'better off dead'],
            'anxiety': ['panic attack', 'can\'t breathe', 'heart racing'],
            'stress': ['breaking point', 'can\'t cope', 'falling apart'],
            'self_harm': ['cut myself', 'hurt myself', 'self harm'],
            'ptsd': ['trauma', 'flashback', 'triggered']
        }
        
        if keyword.lower() in high_severity_keywords.get(condition, []):
            return 2.5  # Increased weight
        else:
            return 1.0
    
    def _analyze_keyword_contexts(self, contexts: List[str], condition: str) -> float:
        """Analyze keyword contexts for additional weighting"""
        try:
            if not contexts:
                return 0.0
            
            amplifiers = {
                'depression': ['always', 'never', 'constantly', 'every day', 'all the time'],
                'anxiety': ['all the time', 'constantly', 'every day', 'non-stop'],
                'stress': ['too much', 'overwhelming', 'can\'t handle']
            }
            
            context_weight = 0.0
            condition_amplifiers = amplifiers.get(condition, [])
            
            for context in contexts:
                for amplifier in condition_amplifiers:
                    if amplifier.lower() in context.lower():
                        context_weight += 0.3  # Increased weight
            
            return min(context_weight, 0.6)  # Increased cap
            
        except Exception:
            return 0.0
    
    def _analyze_voice_characteristics(self, audio_features: Dict) -> Dict:
        """Analyze voice characteristics for mental health indicators"""
        try:
            score = 0.0
            indicators = []
            conditions = {}
            
            # Extract key features
            pitch_mean = audio_features.get('pitch_mean', 0)
            energy = audio_features.get('rms_energy', 0)
            speech_ratio = audio_features.get('speech_ratio', 1.0)
            voice_stability = audio_features.get('voice_stability', 0.5)
            intensity_variation = audio_features.get('voice_intensity_variation', 0)
            silence_ratio = audio_features.get('silence_ratio', 0)
            
            # Depression indicators (enhanced detection)
            depression_score = 0.0
            depression_indicators = []
            
            if pitch_mean < 140 and energy < 0.03:  # Lowered thresholds
                depression_score += 0.5
                depression_indicators.append('Low vocal energy and pitch')
                
            if voice_stability > 0.75:  # Lowered threshold for monotone
                depression_score += 0.3
                depression_indicators.append('Monotone speech pattern')
                
            if silence_ratio > 0.3:  # Lowered threshold
                depression_score += 0.4
                depression_indicators.append('Excessive silence periods')
            
            if depression_score > 0.15:  # Lowered threshold
                conditions['depression'] = {
                    'severity': min(depression_score, 1.0),
                    'confidence': min(depression_score * 100, 100),
                    'voice_features': depression_indicators,
                    'keywords': []
                }
                score += depression_score
                indicators.extend(depression_indicators)
            
            # Anxiety indicators (enhanced detection)
            anxiety_score = 0.0
            anxiety_indicators = []
            
            if pitch_mean > 180:  # Lowered threshold
                anxiety_score += 0.3
                anxiety_indicators.append('Elevated vocal pitch')
                
            if voice_stability < 0.4:  # Increased threshold
                anxiety_score += 0.4
                anxiety_indicators.append('Unstable voice')
                
            if intensity_variation > 1.2:  # Lowered threshold
                anxiety_score += 0.3
                anxiety_indicators.append('High intensity variation')
            
            if anxiety_score > 0.15:  # Lowered threshold
                conditions['anxiety'] = {
                    'severity': min(anxiety_score, 1.0),
                    'confidence': min(anxiety_score * 100, 100),
                    'voice_features': anxiety_indicators,
                    'keywords': []
                }
                score += anxiety_score
                indicators.extend(anxiety_indicators)
            
            # Stress indicators (enhanced detection)
            stress_score = 0.0
            stress_indicators = []
            
            avg_pause = audio_features.get('avg_pause_duration', 0)
            if avg_pause > 1.5:  # Lowered threshold
                stress_score += 0.3
                stress_indicators.append('Long pauses between speech')
                
            if speech_ratio < 0.4:  # Increased threshold
                stress_score += 0.3
                stress_indicators.append('Limited speech activity')
            
            if stress_score > 0.1:  # Lowered threshold
                conditions['stress'] = {
                    'severity': min(stress_score, 1.0),
                    'confidence': min(stress_score * 100, 100),
                    'voice_features': stress_indicators,
                    'keywords': []
                }
                score += stress_score
                indicators.extend(stress_indicators)
            
            return {
                'score': min(score, 1.0),
                'indicators': indicators,
                'conditions': conditions
            }
            
        except Exception as e:
            logger.warning(f"Voice characteristics analysis failed: {e}")
            return {'score': 0.0, 'indicators': [], 'conditions': {}}
    
    def _analyze_audio_only_indicators(self, audio_features: Dict) -> Dict:
        """Fallback analysis using only audio features when transcription fails"""
        try:
            logger.info("Performing audio-only mental health analysis")
            
            voice_analysis = self._analyze_voice_characteristics(audio_features)
            
            primary_concern = 'none_detected'
            if voice_analysis['conditions']:
                primary_concern = max(voice_analysis['conditions'].keys(),
                                    key=lambda x: voice_analysis['conditions'][x]['severity'])
            
            return {
                'mental_health_score': voice_analysis['score'],
                'indicators': voice_analysis['indicators'],
                'detected_conditions': voice_analysis['conditions'],
                'primary_concern': primary_concern,
                'language_analyzed': 'en',
                'analysis_confidence': 0.6,
                'transcription_quality_used': 'failed',
                'analysis_method': 'audio_only'
            }
            
        except Exception as e:
            logger.error(f"Audio-only analysis failed: {e}")
            return {
                'mental_health_score': 0.0, 'indicators': [], 'detected_conditions': {},
                'primary_concern': 'none_detected', 'language_analyzed': 'en',
                'analysis_confidence': 0.0, 'analysis_method': 'failed'
            }
    
    def _determine_primary_concern(self, detected_conditions: Dict) -> str:
        """Determine primary mental health concern with enhanced logic"""
        if not detected_conditions:
            return 'none_detected'
        
        try:
            weighted_scores = {}
            for condition, data in detected_conditions.items():
                severity = data.get('severity', 0)
                confidence = data.get('confidence', 0)
                
                # Enhanced priority weights
                priority_weights = {
                    'self_harm': 2.0,  # Highest priority
                    'depression': 1.5, 
                    'anxiety': 1.3, 
                    'ptsd': 1.4,
                    'stress': 1.1, 
                    'bipolar': 1.2
                }
                
                priority_weight = priority_weights.get(condition, 1.0)
                weighted_score = severity * (confidence / 100) * priority_weight
                weighted_scores[condition] = weighted_score
            
            return max(weighted_scores, key=weighted_scores.get)
            
        except Exception as e:
            logger.warning(f"Primary concern determination failed: {e}")
            return max(detected_conditions.keys(), 
                      key=lambda x: detected_conditions[x].get('severity', 0))
    
    def _calculate_enhanced_analysis_confidence(self, transcription: Dict, 
                                              audio_features: Dict, 
                                              detected_conditions: Dict) -> float:
        """Calculate comprehensive analysis confidence"""
        try:
            confidence = 0.4  # Base confidence
            
            # Transcription quality contribution
            transcription_confidence = transcription.get('confidence', 0.0)
            transcription_quality = transcription.get('transcription_quality', 'poor')
            
            quality_weights = {
                'excellent': 0.3, 'good': 0.25, 'fair': 0.15,
                'poor': 0.05, 'very_poor': 0.0, 'failed': 0.0
            }
            
            confidence += quality_weights.get(transcription_quality, 0.0)
            confidence += transcription_confidence * 0.2
            
            # Audio quality contribution
            duration = audio_features.get('duration', 0)
            speech_ratio = audio_features.get('speech_ratio', 0)
            
            if duration > 10:  # Good duration
                confidence += 0.15
            elif duration > 5:
                confidence += 0.1
            elif duration < 3:
                confidence -= 0.1
            
            if speech_ratio > 0.3:  # Lowered threshold
                confidence += 0.1
            elif speech_ratio < 0.15:
                confidence -= 0.05
            
            # Detection consistency bonus
            if len(detected_conditions) > 1:
                severities = [data.get('severity', 0) for data in detected_conditions.values()]
                if max(severities) - min(severities) < 0.4:  # Consistent severities
                    confidence += 0.1
            
            return float(np.clip(confidence, 0.0, 1.0))
            
        except Exception as e:
            logger.warning(f"Confidence calculation failed: {e}")
            return 0.5
    
    def _fallback_emotion_analysis(self, audio_path: str) -> Dict:
        """Enhanced fallback emotion analysis using audio features"""
        try:
            logger.info("Using fallback emotion analysis based on audio features")
            
            audio, sr = self.load_audio(audio_path)
            features = self.extract_audio_features(audio, sr)
            
            emotions = {'neutral': 0.3}
            
            energy = features.get('rms_energy', 0)
            pitch_mean = features.get('pitch_mean', 0)
            voice_stability = features.get('voice_stability', 0.5)
            intensity_variation = features.get('voice_intensity_variation', 0)
            
            # Enhanced heuristics
            if energy > 0.08 and pitch_mean > 170 and intensity_variation > 0.8:
                emotions.update({'excited': 0.4, 'happy': 0.3, 'neutral': 0.3})
            elif energy < 0.04 and pitch_mean < 140:
                emotions.update({'sad': 0.5, 'tired': 0.3, 'neutral': 0.2})
            elif pitch_mean > 200 and voice_stability < 0.4:
                emotions.update({'anxious': 0.4, 'nervous': 0.3, 'neutral': 0.3})
            elif voice_stability > 0.8 and energy < 0.06:
                emotions.update({'calm': 0.4, 'neutral': 0.6})
            elif intensity_variation > 1.3:
                emotions.update({'angry': 0.3, 'frustrated': 0.3, 'neutral': 0.4})
            else:
                emotions = {'neutral': 0.6, 'calm': 0.4}
            
            top_emotion = max(emotions, key=emotions.get)
            emotional_intensity = self._calculate_emotional_intensity(emotions)
            
            return {
                'emotions': emotions,
                'dominant_emotion': top_emotion,
                'confidence': emotions[top_emotion],
                'model_used': 'enhanced-rule-based-fallback',
                'emotional_intensity': emotional_intensity,
                'analysis_successful': True
            }
            
        except Exception as e:
            logger.error(f"Fallback emotion analysis failed: {str(e)}")
            return {
                'emotions': {'neutral': 1.0},
                'dominant_emotion': 'neutral',
                'confidence': 1.0,
                'model_used': 'default-fallback',
                'emotional_intensity': 0.0,
                'analysis_successful': False
            }
    
    def calculate_enhanced_audio_risk_score(self, features: Dict, emotions: Dict, 
                                          mental_health: Dict, transcription: Dict) -> float:
        """Enhanced risk calculation with comprehensive weighting"""
        try:
            risk_score = 0.0
            
            # Base mental health score (highest weight - 60%)
            mental_health_score = mental_health.get('mental_health_score', 0)
            risk_score += mental_health_score * 0.6  # Increased weight
            
            # Detected conditions with enhanced severity weighting (25%)
            detected_conditions = mental_health.get('detected_conditions', {})
            condition_risk_weights = {
                'self_harm': 0.8,  # Highest risk
                'depression': 0.6, 
                'anxiety': 0.4, 
                'stress': 0.3, 
                'ptsd': 0.5, 
                'bipolar': 0.4
            }
            
            for condition, data in detected_conditions.items():
                condition_weight = condition_risk_weights.get(condition, 0.2)
                severity = data.get('severity', 0)
                confidence = data.get('confidence', 0) / 100  # Convert to 0-1
                
                condition_risk = severity * confidence * condition_weight
                risk_score += condition_risk * 0.25
            
            # Emotion-based risk with intensity weighting (10%)
            emotion_risk_weights = {
                'sadness': 0.5, 'anger': 0.4, 'anxiety': 0.45, 'fear': 0.45,
                'disgust': 0.3, 'surprise': 0.1, 'joy': -0.2, 'happy': -0.2,
                'calm': -0.15, 'neutral': 0.0, 'excited': 0.05
            }
            
            dominant_emotion = emotions.get('dominant_emotion', 'neutral')
            emotion_confidence = emotions.get('confidence', 0)
            emotional_intensity = emotions.get('emotional_intensity', 0)
            
            emotion_risk = emotion_risk_weights.get(dominant_emotion, 0.0)
            emotion_contribution = emotion_risk * emotion_confidence * (1 + emotional_intensity) * 0.1
            risk_score += emotion_contribution
            
            # Voice quality indicators (5%)
            voice_stability = features.get('voice_stability', 0.5)
            speech_ratio = features.get('speech_ratio', 0.5)
            silence_ratio = features.get('silence_ratio', 0)
            
            if voice_stability < 0.3 or voice_stability > 0.9:
                risk_score += 0.02
            
            if speech_ratio < 0.25:  # Lowered threshold
                risk_score += 0.02
                
            if silence_ratio > 0.4:  # Lowered threshold
                risk_score += 0.01
            
            # Quality-based confidence adjustment
            analysis_confidence = mental_health.get('analysis_confidence', 0.5)
            if analysis_confidence < 0.4:
                risk_score *= 0.85  # Reduce risk score for low-confidence analysis
            
            final_risk_score = np.clip(risk_score, 0.0, 1.0)
            
            logger.debug(f"Risk calculation: base={mental_health_score:.3f}, final={final_risk_score:.3f}")
            
            return float(final_risk_score)
            
        except Exception as e:
            logger.error(f"Risk calculation failed: {str(e)}")
            return 0.0
    
    def analyze_audio(self, audio_file_path: str) -> Dict:
        """Comprehensive audio analysis with enhanced error handling"""
        if not os.path.exists(audio_file_path):
            logger.error(f"Audio file not found: {audio_file_path}")
            return self._empty_audio_analysis()
        
        analysis_start_time = time.time()
        
        try:
            logger.info(f"Starting comprehensive audio analysis for: {audio_file_path}")
            
            # Load and validate audio
            audio, sr = self.load_audio(audio_file_path)
            
            if len(audio) == 0:
                raise ValueError("Audio file is empty or contains no audio data")
            
            # Extract comprehensive features
            logger.debug("Extracting audio features...")
            features = self.extract_audio_features(audio, sr)
            
            # Voice activity detection
            logger.debug("Analyzing voice activity...")
            vad_results = self.detect_voice_activity(audio, sr)
            features.update(vad_results)
            
            # Enhanced speech-to-text
            logger.debug("Transcribing audio...")
            transcription = self.transcribe_audio(audio_file_path)
            
            # Emotion analysis
            logger.debug("Analyzing emotions...")
            emotions = self.analyze_emotion_from_audio(audio_file_path)
            
            # Mental health analysis
            logger.debug("Analyzing mental health indicators...")
            mental_health = self.analyze_mental_health_indicators(transcription, features)
            
            # Enhanced risk assessment
            logger.debug("Calculating risk score...")
            risk_score = self.calculate_enhanced_audio_risk_score(
                features, emotions, mental_health, transcription
            )
            
            # Generate recommendations
            recommendations = self._generate_audio_recommendations(mental_health, emotions, features)
            
            # Calculate processing time
            processing_time = time.time() - analysis_start_time
            
            logger.info(f"Audio analysis completed successfully in {processing_time:.2f}s")
            
            return {
                'file_path': audio_file_path,
                'audio_features': features,
                'transcription': transcription,
                'emotions': emotions,
                'mental_health_analysis': mental_health,
                'risk_score': risk_score,
                'primary_concern': mental_health['primary_concern'],
                'language_detected': 'en',
                'analysis_confidence': mental_health.get('analysis_confidence', 0.5),
                'analysis_success': True,
                'recommendations': recommendations,
                'processing_time_seconds': processing_time,
                'analysis_timestamp': datetime.now().isoformat(),
                'model_versions': {
                    'speech_to_text': 'whisper-base',
                    'emotion_model': 'wav2vec2' if self.emotion_classifier else 'rule-based'
                }
            }
            
        except Exception as e:
            processing_time = time.time() - analysis_start_time
            logger.error(f"Audio analysis failed after {processing_time:.2f}s: {str(e)}")
            
            return {
                **self._empty_audio_analysis(),
                'error': str(e),
                'analysis_success': False,
                'processing_time_seconds': processing_time,
                'analysis_timestamp': datetime.now().isoformat()
            }
    
    def _generate_audio_recommendations(self, mental_health: Dict, emotions: Dict, features: Dict) -> List[str]:
        """Generate comprehensive recommendations based on audio analysis"""
        try:
            recommendations = []
            
            primary_concern = mental_health.get('primary_concern', 'none_detected')
            dominant_emotion = emotions.get('dominant_emotion', 'neutral')
            emotional_intensity = emotions.get('emotional_intensity', 0)
            
            # Enhanced condition-specific recommendations
            if primary_concern == 'depression':
                recommendations.extend([
                    "Consider speaking with a mental health professional about persistent sadness or low mood",
                    "Try to maintain regular social connections and conversations",
                    "Voice exercises, singing, or reading aloud can help improve vocal energy and mood",
                    "Establish a daily routine to provide structure and purpose"
                ])
            elif primary_concern == 'anxiety':
                recommendations.extend([
                    "Practice slow, deep breathing exercises to calm your voice and mind",
                    "Try progressive muscle relaxation to reduce physical tension in your voice",
                    "Consider speaking at a slightly slower pace to help reduce anxiety",
                    "Limit caffeine intake which can increase anxiety symptoms"
                ])
            elif primary_concern == 'stress':
                recommendations.extend([
                    "Take regular breaks during long conversations or phone calls",
                    "Practice vocal relaxation techniques and gentle humming",
                    "Consider stress management techniques like meditation or mindfulness",
                    "Identify and address sources of stress where possible"
                ])
            elif primary_concern == 'self_harm':
                recommendations.extend([
                    "Please reach out to a mental health professional immediately",
                    "Contact a crisis helpline: National Suicide Prevention Lifeline 988",
                    "Remove any items that could be used for self-harm from your immediate environment",
                    "Stay with trusted friends or family members"
                ])
            elif primary_concern == 'ptsd':
                recommendations.extend([
                    "Consider trauma-informed therapy approaches like EMDR or CPT",
                    "Practice grounding techniques when feeling overwhelmed",
                    "Develop a support network you can talk to regularly",
                    "Learn about trauma responses to better understand your experiences"
                ])
            
            # Emotion-specific recommendations
            if dominant_emotion in ['sadness', 'anger'] and emotional_intensity > 1.0:
                recommendations.append("Consider professional counseling to address intense emotional experiences")
            
            # Voice quality recommendations
            voice_stability = features.get('voice_stability', 0.5)
            speech_ratio = features.get('speech_ratio', 0.5)
            silence_ratio = features.get('silence_ratio', 0)
            
            if voice_stability < 0.3:
                recommendations.append("Voice instability detected - consider relaxation techniques or vocal exercises")
            elif voice_stability > 0.9:
                recommendations.append("Very monotone speech detected - try varying your vocal expression")
            
            if speech_ratio < 0.3:
                recommendations.append("Limited speech activity - consider engaging in more conversations or vocal exercises")
            
            if silence_ratio > 0.5:
                recommendations.append("Long periods of silence detected - this may indicate withdrawal or difficulty expressing thoughts")
            
            # General wellness recommendations
            recommendations.extend([
                "Maintain regular sleep schedule to support overall mental health",
                "Stay connected with friends and family through regular conversations",
                "Practice mindfulness and self-awareness of your emotional state",
                "Consider keeping a mood journal to track patterns"
            ])
            
            return recommendations[:8]  # Limit to top 8 recommendations
            
        except Exception as e:
            logger.warning(f"Recommendation generation failed: {e}")
            return [
                "Consider speaking with a mental health professional",
                "Practice self-care and stress management techniques",
                "Maintain regular social connections"
            ]
    
    def _empty_audio_analysis(self) -> Dict:
        """Return empty audio analysis structure with all required fields"""
        return {
            'file_path': '',
            'audio_features': {
                'duration': 0.0, 'rms_energy': 0.0, 'pitch_mean': 0.0,
                'voice_stability': 0.5, 'speech_ratio': 0.0
            },
            'transcription': {
                'transcription': '', 'confidence': 0.0, 'language': 'en',
                'word_count': 0, 'transcription_quality': 'failed'
            },
            'emotions': {
                'emotions': {'neutral': 1.0}, 'dominant_emotion': 'neutral', 
                'confidence': 1.0, 'emotional_intensity': 0.0
            },
            'mental_health_analysis': {
                'mental_health_score': 0.0, 'indicators': [], 'detected_conditions': {},
                'primary_concern': 'none_detected', 'analysis_confidence': 0.0,
                'analysis_method': 'failed'
            },
            'risk_score': 0.0,
            'primary_concern': 'none_detected',
            'language_detected': 'en',
            'analysis_confidence': 0.0,
            'analysis_success': False,
            'recommendations': [
                "Unable to analyze audio - please ensure file is valid",
                "Try recording in a quiet environment with clear speech",
                "Consider professional consultation if you have concerns"
            ],
            'processing_time_seconds': 0.0,
            'analysis_timestamp': datetime.now().isoformat(),
                        'model_versions': {
                'speech_to_text': 'unavailable',
                'emotion_model': 'unavailable'
            }
        }

# Additional utility functions and test code
def validate_audio_analyzer():
    """Validate audio analyzer initialization"""
    try:
        analyzer = AudioAnalyzer()
        logger.info("Audio analyzer validation successful")
        return True
    except Exception as e:
        logger.error(f"Audio analyzer validation failed: {e}")
        return False

# Test function for the complete audio analyzer
if __name__ == "__main__":
    import time
    from datetime import datetime
    
    # Test the audio analyzer
    print("=" * 60)
    print("MENTAL HEALTH AUDIO ANALYZER - COMPREHENSIVE TEST")
    print("=" * 60)
    
    try:
        # Initialize analyzer
        print("1. Initializing Audio Analyzer...")
        analyzer = AudioAnalyzer()
        print(" Audio analyzer initialized successfully!")
        
        # Test with a sample file (if available)
        print("\n2. Testing Analysis Functions...")
        
        # Test feature extraction with dummy audio
        print("   - Testing feature extraction...")
        dummy_audio = np.random.randn(16000)  # 1 second of dummy audio
        features = analyzer.extract_audio_features(dummy_audio, 16000)
        print(f"    Extracted {len(features)} features")
        
        # Test voice activity detection
        print("   - Testing voice activity detection...")
        vad_results = analyzer.detect_voice_activity(dummy_audio, 16000)
        print(f"    VAD completed, speech ratio: {vad_results['speech_ratio']:.2f}")
        
        print("\n3. Testing Mental Health Analysis...")
        
        # Test mental health keyword detection
        test_transcription = {
            'transcription': 'I feel hopeless and very sad, cannot sleep well, want to hurt myself',
            'language': 'en',
            'confidence': 0.8,
            'transcription_quality': 'good'
        }
        
        test_features = {
            'duration': 10.0, 'rms_energy': 0.05, 'pitch_mean': 150,
            'voice_stability': 0.6, 'speech_ratio': 0.7
        }
        
        mh_analysis = analyzer.analyze_mental_health_indicators(test_transcription, test_features)
        print(f"    Detected conditions: {list(mh_analysis['detected_conditions'].keys())}")
        print(f"    Primary concern: {mh_analysis['primary_concern']}")
        print(f"    Mental health score: {mh_analysis['mental_health_score']:.2f}")
        
        print("\n4. Testing Emotion Analysis...")
        
        # Test fallback emotion analysis
        with tempfile.NamedTemporaryFile(suffix='.wav', delete=False) as tmp_file:
            # Create a simple sine wave
            import soundfile as sf
            sample_rate = 16000
            duration = 2  # seconds
            frequency = 440  # A4 note
            t = np.linspace(0, duration, duration * sample_rate, False)
            audio_data = 0.3 * np.sin(2 * np.pi * frequency * t)
            
            sf.write(tmp_file.name, audio_data, sample_rate)
            
            emotions = analyzer._fallback_emotion_analysis(tmp_file.name)
            print(f"    Dominant emotion: {emotions['dominant_emotion']}")
            print(f"    Confidence: {emotions['confidence']:.2f}")
            print(f"    Emotional intensity: {emotions['emotional_intensity']:.2f}")
            
            # Clean up
            os.unlink(tmp_file.name)
        
        print("\n5. Performance Summary:")
        print("    All core functions operational")
        print("    English-only support active")
        print("    Mental health detection ready")
        print("    Emotion analysis functional")
        print("    Risk assessment operational")
        
        print(f"\n AUDIO ANALYZER READY FOR PRODUCTION!")
        print(f"    Features: Enhanced keyword detection with {len(analyzer.mental_health_keywords)} condition categories")
        print(f"    Language: English optimized")
        print(f"    Models: Whisper-base, Wav2Vec2 emotion (with fallback)")
        print(f"    Performance: Optimized for mental health analysis")
        
    except Exception as e:
        print(f" Test failed: {str(e)}")
        import traceback
        traceback.print_exc()
    
    print("=" * 60)

